{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# theme = 'plotly_dark'\n",
    "theme = 'seaborn'\n",
    "#theme = 'plotly'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import torch\n",
    "import numpy as np\n",
    "import plotly.graph_objs as go\n",
    "import json\n",
    "title = 'Bouts v. Model Pred'\n",
    "\n",
    "path_to_model_def = '/home/kuba/Projects/MedicationTakingData/resmodel' #this is were the .py file is \n",
    "path_to_dir_with_model_pt_file = '/home/kuba/Projects/MedicationTakingData/resmodel/res_search_00/res_search_00_7'\n",
    "\n",
    "#the watch and recoding we willbe evaling\n",
    "WATCH_DIR = '/home/kuba/Documents/Data/Raw/Listerine/3_final/16'\n",
    "recording = '2024-03-24_13_10_54'\n",
    "\n",
    "HERTZ = 100\n",
    "ACTIVITY_NAME_TO_CLASS_INDEX_MAPPING = {\n",
    "    'water':0,\n",
    "    'listerine':1,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_window(df, start, window_size):\n",
    "    \"\"\"\n",
    "    Prepare a window of accelerometer and gyroscope data for the model.\n",
    "    \"\"\"\n",
    "    window = df.iloc[start:start + window_size]\n",
    "    X_acc = torch.tensor([window[col].values for col in ['x_acc', 'y_acc', 'z_acc']], dtype=torch.float32)\n",
    "    X_gyro = torch.tensor([window[col].values for col in ['x_gyro', 'y_gyro', 'z_gyro']], dtype=torch.float32)\n",
    "    X_combined = torch.cat([X_acc, X_gyro]).unsqueeze(0)\n",
    "    # Combine [1, 6, window_size]\n",
    "    return torch.cat((X_acc, X_gyro), dim=0).unsqueeze(0)\n",
    "\n",
    "def smooth_predictions(prediction_sum, counts):\n",
    "    \"\"\"\n",
    "    Smooth predictions by averaging, handling divisions by zero.\n",
    "    \"\"\"\n",
    "    mask = counts > 0\n",
    "    averaged_predictions = np.zeros_like(prediction_sum)\n",
    "    averaged_predictions[mask] = prediction_sum[mask] / counts[mask]\n",
    "    return averaged_predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_txt(dir):\n",
    "    txt_path = os.path.join(dir, f'desc.txt')\n",
    "    with open(txt_path, 'r') as f:\n",
    "        content = f.read()\n",
    "    dic = eval(content)\n",
    "    return dic['window_size'], dic['stride'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#takes in label a to tensor for ML\n",
    "def json_to_tensor(labels_x, acc_len_x, acc_x):\n",
    "    y_new = torch.zeros(acc_len_x)-1\n",
    "\n",
    "    bouts = []\n",
    "    for hand in labels_x:\n",
    "        for action in labels_x[hand]:\n",
    "            for bout in labels_x[hand][action]:\n",
    "                y_new[(acc_x.timestamp > bout['start']) & (acc_x.timestamp < bout['end'])] = (ACTIVITY_NAME_TO_CLASS_INDEX_MAPPING[action] * 20 + 15)\n",
    "    return y_new\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_preprocess_data(recording_dir):\n",
    "    acc = pd.read_csv(f'{recording_dir}/acceleration.csv', skiprows=1)\n",
    "    acc['timestamp'] = (acc['timestamp'] - acc['timestamp'].iloc[0]) * 1e-9\n",
    "    \n",
    "    gyro = pd.read_csv(f'{recording_dir}/gyroscope.csv', skiprows=1)\n",
    "    gyro['timestamp'] = (gyro['timestamp'] - gyro['timestamp'].iloc[0]) * 1e-9\n",
    "    \n",
    "    # interpolate gyro data to match acc timestamps\n",
    "    gyro_interp = pd.DataFrame()\n",
    "    for axis in ['x', 'y', 'z']:\n",
    "        gyro_interp[axis] = np.interp(acc['timestamp'], gyro['timestamp'], gyro[axis])\n",
    "    \n",
    "    # combine acc and gyro data\n",
    "    sensor_data = pd.DataFrame()\n",
    "    sensor_data['timestamp'] = acc['timestamp']\n",
    "    sensor_data['x_acc'] = acc['x']\n",
    "    sensor_data['y_acc'] = acc['y']\n",
    "    sensor_data['z_acc'] = acc['z']\n",
    "    sensor_data['x_gyro'] = gyro_interp['x']\n",
    "    sensor_data['y_gyro'] = gyro_interp['y']\n",
    "    sensor_data['z_gyro'] = gyro_interp['z']\n",
    "    \n",
    "    return sensor_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bouts(recording_dir):\n",
    "    all_data = []\n",
    "    all_labels = []\n",
    "    all_starts = []\n",
    "    all_ends = []\n",
    "    \n",
    "    label_mapping = [\n",
    "        ('left', 'water', 0.0),\n",
    "        ('left', 'listerine', 1.0),\n",
    "        ('right', 'water', 0.0),\n",
    "        ('right', 'listerine', 1.0)\n",
    "    ]\n",
    "    \n",
    "    with open(os.path.join(recording_dir, 'labels.json')) as f:\n",
    "        labels = json.load(f)\n",
    "        \n",
    "    acc = pd.read_csv(os.path.join(recording_dir, 'acceleration.csv'), skiprows=1)\n",
    "    gyro = pd.read_csv(os.path.join(recording_dir, 'gyroscope.csv'), skiprows=1)\n",
    "    \n",
    "    #convert the timestap to sec\n",
    "    acc['timestamp'] = (acc['timestamp'] - acc['timestamp'].iloc[0]) * 1e-9\n",
    "    gyro['timestamp'] = (gyro['timestamp'] - gyro['timestamp'].iloc[0]) * 1e-9\n",
    "    \n",
    "    merged_data = pd.merge_asof(acc, gyro, on='timestamp', \n",
    "                                suffixes=('_acc', '_gyro'))\n",
    "    \n",
    "    for side, liquid, label_value in label_mapping:\n",
    "        if side in labels and liquid in labels[side]:\n",
    "            for bout in labels[side][liquid]:\n",
    "                start_time = bout['start'] \n",
    "                end_time = bout['end']\n",
    "                # Extract data for this bout\n",
    "                bout_data = merged_data[(merged_data['timestamp'] >= start_time) & (merged_data['timestamp'] <= end_time)].copy()\n",
    "                \n",
    "                start_index = (merged_data[\"timestamp\"] < start_time).sum()\n",
    "                end_index = start_index + len(bout_data) \n",
    "\n",
    "                if len(bout_data) > 0:\n",
    "                    all_data.append(bout_data)\n",
    "                    all_labels.append(label_value)\n",
    "                    all_starts.append(start_index)\n",
    "                    all_ends.append(end_index)\n",
    "\n",
    "    return all_data, all_labels, all_starts, all_ends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_preds(model, device, bout, label, window_size, stride):\n",
    "    assert 'timestamp' in bout.columns, \"Sensor data must include 'timestamp' column.\"\n",
    "    # y_df = pd.DataFrame(label, columns=['labels'])\n",
    "    # df = pd.concat([bout, y_df], axis=1)\n",
    "    \n",
    "    prediction_sum = np.zeros(len(bout))\n",
    "    counts = np.zeros(len(bout))\n",
    "    \n",
    "    for i in range(0, len(bout) - window_size + 1, stride):\n",
    "        X_combined = preprocess_window(bout, i, window_size).to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            logits = torch.sigmoid(model(X_combined)).cpu().numpy()[0]\n",
    "            prediction_sum[i:i + window_size] += logits\n",
    "            counts[i:i + window_size] += 1\n",
    "    \n",
    "    return prediction_sum, counts\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/kuba/Projects/MedicationTakingData/resmodel/res_search_00/res_search_00_7/res_search_00_7_bestF1.pth\n",
      "Using device: cuda:1\n",
      "Processing recording: 2024-03-24_13_10_54\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1215395/1969281986.py:6: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:274.)\n",
      "  X_acc = torch.tensor([window[col].values for col in ['x_acc', 'y_acc', 'z_acc']], dtype=torch.float32)\n"
     ]
    }
   ],
   "source": [
    "sys.path.append(path_to_model_def)\n",
    "head_tail = os.path.split(path_to_dir_with_model_pt_file)\n",
    "model_path = os.path.join(path_to_dir_with_model_pt_file, f'{head_tail[1]}_bestF1.pth')\n",
    "\n",
    "\n",
    "#get model and meta data\n",
    "window_size, stride = read_txt(path_to_dir_with_model_pt_file)\n",
    "print(model_path)\n",
    "\n",
    "# load model\n",
    "model = torch.load(model_path)\n",
    "model.eval()\n",
    "device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "model = model.to(device)\n",
    "\n",
    "\n",
    "# for each recording do the viz recording\n",
    "if recording == '*':\n",
    "    recordings = sorted(os.listdir(WATCH_DIR))\n",
    "else:\n",
    "    recordings = [recording]\n",
    "\n",
    "for rec in recordings:\n",
    "    if rec == '.DS_Store':\n",
    "        continue\n",
    "        \n",
    "    print(f\"Processing recording: {rec}\")\n",
    "    recording_dir = f'{WATCH_DIR}/{rec}'\n",
    "    #get bouts\n",
    "    sensor_data = load_and_preprocess_data(recording_dir)\n",
    "    all_bouts, all_labels, all_starts, all_ends = get_bouts(recording_dir)\n",
    "\n",
    "    yhat_full_len = np.zeros(len(sensor_data)) #will ve all zeros then will add in the smoothed preds at the right indexes\n",
    "    y_full_len = np.zeros(len(sensor_data)) #will ve all zeros then will add in the true y at the right indexes\n",
    "\n",
    "    for bout, label, start, end in zip(all_bouts, all_labels, all_starts, all_ends):\n",
    "        pred_sum, pred_count = get_preds(model, device, bout, label, window_size, stride) #partition inot widows and combine with the true value\n",
    "        avg_preds = smooth_predictions(pred_sum, pred_count)\n",
    "\n",
    "        yhat_full_len[start:end] = avg_preds  * 20 + 10\n",
    "        y_full_len[start:end] = label  * 20 + 10\n",
    "\n",
    "    \n",
    "    fig = go.Figure()\n",
    "    sensor_cols = ['x_acc', 'y_acc', 'z_acc']\n",
    "    for col in sensor_cols:\n",
    "        fig.add_trace(go.Scatter(\n",
    "            x=sensor_data['timestamp'], y=sensor_data[col],\n",
    "            name=f'{col}',\n",
    "            mode='lines', opacity=0.7\n",
    "        ))\n",
    "    \n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=sensor_data['timestamp'], y=y_full_len,\n",
    "        name='True Labels', mode='lines',\n",
    "        line=dict(color='black', width=2)\n",
    "    ))\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=sensor_data['timestamp'], y=yhat_full_len,\n",
    "        name='Predictions', mode='lines',\n",
    "        line=dict(color='red', width=3, dash='dash')\n",
    "    ))\n",
    "    \n",
    "    fig.update_layout(\n",
    "        title=title, xaxis_title='Time (s)',\n",
    "        yaxis_title='Value', template='plotly',\n",
    "        legend=dict(yanchor=\"top\", y=0.99, xanchor=\"left\", x=0.01)\n",
    "    )\n",
    "    fig.show(renderer='browser')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
